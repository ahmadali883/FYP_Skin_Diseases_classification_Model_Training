{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.10.12","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"nvidiaTeslaT4","dataSources":[{"sourceId":10714327,"sourceType":"datasetVersion","datasetId":6641082}],"dockerImageVersionId":30887,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":true}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"!pip install tensorflow-addons==0.16.1\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-03-01T10:46:06.396288Z","iopub.execute_input":"2025-03-01T10:46:06.396689Z","iopub.status.idle":"2025-03-01T10:46:10.992967Z","shell.execute_reply.started":"2025-03-01T10:46:06.396629Z","shell.execute_reply":"2025-03-01T10:46:10.991918Z"}},"outputs":[{"name":"stdout","text":"Collecting tensorflow-addons==0.16.1\n  Downloading tensorflow_addons-0.16.1-cp310-cp310-manylinux_2_12_x86_64.manylinux2010_x86_64.whl.metadata (1.8 kB)\nRequirement already satisfied: typeguard>=2.7 in /usr/local/lib/python3.10/dist-packages (from tensorflow-addons==0.16.1) (4.4.1)\nRequirement already satisfied: typing-extensions>=4.10.0 in /usr/local/lib/python3.10/dist-packages (from typeguard>=2.7->tensorflow-addons==0.16.1) (4.12.2)\nDownloading tensorflow_addons-0.16.1-cp310-cp310-manylinux_2_12_x86_64.manylinux2010_x86_64.whl (1.1 MB)\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.1/1.1 MB\u001b[0m \u001b[31m17.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0ma \u001b[36m0:00:01\u001b[0m\n\u001b[?25hInstalling collected packages: tensorflow-addons\nSuccessfully installed tensorflow-addons-0.16.1\n","output_type":"stream"}],"execution_count":1},{"cell_type":"code","source":"# Import Libraries\nimport os\nimport cv2\nimport numpy as np\nimport pandas as pd\nimport matplotlib.pyplot as plt\nimport seaborn as sns\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.utils.class_weight import compute_class_weight\nfrom sklearn.metrics import classification_report, confusion_matrix\n\nimport tensorflow as tf\nfrom tensorflow.keras.applications import VGG19\nfrom tensorflow.keras.models import Sequential\nfrom tensorflow.keras.layers import Dense, Dropout, Flatten, Lambda, RandomFlip, RandomRotation, RandomZoom, RandomContrast\nfrom tensorflow.keras.optimizers import Adam\nfrom tensorflow.keras.callbacks import EarlyStopping, ReduceLROnPlateau\nfrom tensorflow.keras.regularizers import l2\n\n# Configuration\nIMG_SIZE = (180, 180)\nBATCH_SIZE = 128  #32\nEPOCHS = 100  #100\nSEED = 42\nCLASS_NAMES = ['Eczema', 'ACNE', 'Psoriasis']\nBASE_PATH = \"/kaggle/input/acne-psoriasis-eczema-dataset/Acne_Psoriasis_Eczema_Dataset/\"\n\n# 1. Data Preparation\ndef create_dataframe():\n    data_dict = {\"image_path\": [], \"target\": []}\n    \n    class_info = [\n        (\"1. Eczema 1677\", 0),\n        (\"ACNE\", 1),\n        (\"Psoriasis\", 2)\n    ]\n    \n    for class_dir, label in class_info:\n        full_path = os.path.join(BASE_PATH, class_dir)\n        if not os.path.exists(full_path):\n            raise FileNotFoundError(f\"Directory not found: {full_path}\")\n            \n        for img_file in os.listdir(full_path):\n            if img_file.lower().endswith(('.png', '.jpg', '.jpeg')):\n                img_path = os.path.join(full_path, img_file)\n                data_dict[\"image_path\"].append(img_path)\n                data_dict[\"target\"].append(label)\n                \n    return pd.DataFrame(data_dict)\n\ndf = create_dataframe()\nprint(\"Class distribution:\\n\", df['target'].value_counts())\n\n# 2. Handle Class Imbalance\nclass_weights = compute_class_weight('balanced', classes=np.unique(df['target']), y=df['target'])\nclass_weights = dict(enumerate(class_weights))\nprint(\"\\nClass weights:\", class_weights)\n\n# 3. Image Loading and Preprocessing\ndef load_images(df):\n    images = []\n    labels = []\n    \n    for idx, row in df.iterrows():\n        try:\n            img = cv2.imread(row['image_path'])\n            if img is None:\n                raise ValueError(\"Could not read image\")\n                \n            img = cv2.cvtColor(img, cv2.COLOR_BGR2RGB)\n            img = cv2.resize(img, IMG_SIZE)\n            images.append(img)\n            labels.append(row['target'])\n        except Exception as e:\n            print(f\"Error processing {row['image_path']}: {str(e)}\")\n    \n    return np.array(images, dtype=np.float32), np.array(labels)\n\nimages, labels = load_images(df)\n\n# 4. Data Splitting\nX_train, X_test, y_train, y_test = train_test_split(\n    images, \n    labels,\n    test_size=0.2,\n    stratify=labels,\n    random_state=SEED\n)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-03-01T10:46:10.994287Z","iopub.execute_input":"2025-03-01T10:46:10.994625Z","iopub.status.idle":"2025-03-01T10:49:45.993100Z","shell.execute_reply.started":"2025-03-01T10:46:10.994591Z","shell.execute_reply":"2025-03-01T10:49:45.992088Z"}},"outputs":[{"name":"stdout","text":"Class distribution:\n target\n0    9327\n2    3812\n1    3198\nName: count, dtype: int64\n\nClass weights: {0: 0.5838604767520817, 1: 1.7028351052741297, 2: 1.4285589366911509}\n","output_type":"stream"}],"execution_count":2},{"cell_type":"code","source":"# 5. Model Architecture with Separate Augmentation (FIXED)\ndef build_model():\n    # Create base VGG19 model\n    base_model = VGG19(\n        weights='imagenet',\n        include_top=False,\n        input_shape=(IMG_SIZE[0], IMG_SIZE[1], 3)\n    )\n    base_model.trainable = False\n\n    # Custom preprocessing layer for VGG19\n    class VGGPreprocess(tf.keras.layers.Layer):\n        def call(self, inputs):\n            return tf.keras.applications.vgg19.preprocess_input(inputs)\n\n    # Build model\n    model = Sequential([\n        VGGPreprocess(),\n        base_model,\n        Flatten(),\n        Dense(512, activation='relu', kernel_regularizer=l2(0.01)),\n        Dropout(0.5),\n        Dense(256, activation='relu', kernel_regularizer=l2(0.01)),\n        Dropout(0.3),\n        Dense(3, activation='softmax')\n    ])\n\n    model.compile(\n        optimizer=Adam(learning_rate=1e-4),\n        loss='sparse_categorical_crossentropy',\n        metrics=['accuracy']\n    )\n    return model\n\n# Create augmentation layer\naugmentation = Sequential([\n    RandomFlip(\"horizontal_and_vertical\"),\n    RandomRotation(0.2),\n    RandomZoom(0.15),\n    RandomContrast(0.1)\n])\n\n# Create optimized data pipeline\ndef create_dataset(images, labels, training=False):\n    dataset = tf.data.Dataset.from_tensor_slices((images, labels))\n    \n    if training:\n        dataset = dataset.map(\n            lambda x, y: (augmentation(x, training=True), y),\n            num_parallel_calls=tf.data.AUTOTUNE\n        )\n    \n    return dataset.batch(BATCH_SIZE).prefetch(tf.data.AUTOTUNE)\n\n# Create datasets with proper device placement\nwith tf.device('/CPU:0'):  # Force CPU preprocessing\n    train_dataset = create_dataset(X_train, y_train, training=True)\n    test_dataset = create_dataset(X_test, y_test)\n\n# 6. Define Callbacks First\ncallbacks = [\n    EarlyStopping(\n        monitor='val_loss',\n        patience=20,\n        restore_best_weights=True,\n        verbose=1\n    ),\n    ReduceLROnPlateau(\n        monitor='val_loss',\n        factor=0.2,\n        patience=20,\n        verbose=1\n    )\n]\n\n# Then proceed with model building and training\nmodel = build_model()\nmodel.summary()\n\nhistory = model.fit(\n    train_dataset,\n    validation_data=test_dataset,\n    epochs=EPOCHS,\n    class_weight=class_weights,\n    callbacks=callbacks,  # Now properly defined\n    verbose=1\n)\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-03-01T10:49:45.994511Z","iopub.execute_input":"2025-03-01T10:49:45.995118Z"}},"outputs":[{"name":"stdout","text":"Downloading data from https://storage.googleapis.com/tensorflow/keras-applications/vgg19/vgg19_weights_tf_dim_ordering_tf_kernels_notop.h5\n\u001b[1m80134624/80134624\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 0us/step\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"\u001b[1mModel: \"sequential_1\"\u001b[0m\n","text/html":"<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\">Model: \"sequential_1\"</span>\n</pre>\n"},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"┏━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━┓\n┃\u001b[1m \u001b[0m\u001b[1mLayer (type)                        \u001b[0m\u001b[1m \u001b[0m┃\u001b[1m \u001b[0m\u001b[1mOutput Shape               \u001b[0m\u001b[1m \u001b[0m┃\u001b[1m \u001b[0m\u001b[1m        Param #\u001b[0m\u001b[1m \u001b[0m┃\n┡━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━┩\n│ vgg_preprocess (\u001b[38;5;33mVGGPreprocess\u001b[0m)       │ ?                           │     \u001b[38;5;34m0\u001b[0m (unbuilt) │\n├──────────────────────────────────────┼─────────────────────────────┼─────────────────┤\n│ vgg19 (\u001b[38;5;33mFunctional\u001b[0m)                   │ (None, 5, 5, 512)           │      \u001b[38;5;34m20,024,384\u001b[0m │\n├──────────────────────────────────────┼─────────────────────────────┼─────────────────┤\n│ flatten (\u001b[38;5;33mFlatten\u001b[0m)                    │ ?                           │     \u001b[38;5;34m0\u001b[0m (unbuilt) │\n├──────────────────────────────────────┼─────────────────────────────┼─────────────────┤\n│ dense (\u001b[38;5;33mDense\u001b[0m)                        │ ?                           │     \u001b[38;5;34m0\u001b[0m (unbuilt) │\n├──────────────────────────────────────┼─────────────────────────────┼─────────────────┤\n│ dropout (\u001b[38;5;33mDropout\u001b[0m)                    │ ?                           │     \u001b[38;5;34m0\u001b[0m (unbuilt) │\n├──────────────────────────────────────┼─────────────────────────────┼─────────────────┤\n│ dense_1 (\u001b[38;5;33mDense\u001b[0m)                      │ ?                           │     \u001b[38;5;34m0\u001b[0m (unbuilt) │\n├──────────────────────────────────────┼─────────────────────────────┼─────────────────┤\n│ dropout_1 (\u001b[38;5;33mDropout\u001b[0m)                  │ ?                           │     \u001b[38;5;34m0\u001b[0m (unbuilt) │\n├──────────────────────────────────────┼─────────────────────────────┼─────────────────┤\n│ dense_2 (\u001b[38;5;33mDense\u001b[0m)                      │ ?                           │     \u001b[38;5;34m0\u001b[0m (unbuilt) │\n└──────────────────────────────────────┴─────────────────────────────┴─────────────────┘\n","text/html":"<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">┏━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━┓\n┃<span style=\"font-weight: bold\"> Layer (type)                         </span>┃<span style=\"font-weight: bold\"> Output Shape                </span>┃<span style=\"font-weight: bold\">         Param # </span>┃\n┡━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━┩\n│ vgg_preprocess (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">VGGPreprocess</span>)       │ ?                           │     <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> (unbuilt) │\n├──────────────────────────────────────┼─────────────────────────────┼─────────────────┤\n│ vgg19 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Functional</span>)                   │ (None, 5, 5, 512)           │      <span style=\"color: #00af00; text-decoration-color: #00af00\">20,024,384</span> │\n├──────────────────────────────────────┼─────────────────────────────┼─────────────────┤\n│ flatten (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Flatten</span>)                    │ ?                           │     <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> (unbuilt) │\n├──────────────────────────────────────┼─────────────────────────────┼─────────────────┤\n│ dense (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dense</span>)                        │ ?                           │     <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> (unbuilt) │\n├──────────────────────────────────────┼─────────────────────────────┼─────────────────┤\n│ dropout (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dropout</span>)                    │ ?                           │     <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> (unbuilt) │\n├──────────────────────────────────────┼─────────────────────────────┼─────────────────┤\n│ dense_1 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dense</span>)                      │ ?                           │     <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> (unbuilt) │\n├──────────────────────────────────────┼─────────────────────────────┼─────────────────┤\n│ dropout_1 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dropout</span>)                  │ ?                           │     <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> (unbuilt) │\n├──────────────────────────────────────┼─────────────────────────────┼─────────────────┤\n│ dense_2 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dense</span>)                      │ ?                           │     <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> (unbuilt) │\n└──────────────────────────────────────┴─────────────────────────────┴─────────────────┘\n</pre>\n"},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"\u001b[1m Total params: \u001b[0m\u001b[38;5;34m20,024,384\u001b[0m (76.39 MB)\n","text/html":"<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Total params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">20,024,384</span> (76.39 MB)\n</pre>\n"},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"\u001b[1m Trainable params: \u001b[0m\u001b[38;5;34m0\u001b[0m (0.00 B)\n","text/html":"<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Trainable params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> (0.00 B)\n</pre>\n"},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"\u001b[1m Non-trainable params: \u001b[0m\u001b[38;5;34m20,024,384\u001b[0m (76.39 MB)\n","text/html":"<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Non-trainable params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">20,024,384</span> (76.39 MB)\n</pre>\n"},"metadata":{}},{"name":"stdout","text":"Epoch 1/100\n\u001b[1m103/103\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m146s\u001b[0m 1s/step - accuracy: 0.4360 - loss: 17.3847 - val_accuracy: 0.6169 - val_loss: 13.6599 - learning_rate: 1.0000e-04\nEpoch 2/100\n\u001b[1m103/103\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m95s\u001b[0m 922ms/step - accuracy: 0.5132 - loss: 14.0644 - val_accuracy: 0.6105 - val_loss: 12.9531 - learning_rate: 1.0000e-04\nEpoch 3/100\n\u001b[1m103/103\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m95s\u001b[0m 920ms/step - accuracy: 0.5242 - loss: 13.0508 - val_accuracy: 0.6190 - val_loss: 12.2934 - learning_rate: 1.0000e-04\nEpoch 4/100\n\u001b[1m103/103\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m95s\u001b[0m 927ms/step - accuracy: 0.5385 - loss: 12.3092 - val_accuracy: 0.6169 - val_loss: 11.6357 - learning_rate: 1.0000e-04\nEpoch 5/100\n\u001b[1m103/103\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m96s\u001b[0m 928ms/step - accuracy: 0.5397 - loss: 11.6295 - val_accuracy: 0.6239 - val_loss: 10.9847 - learning_rate: 1.0000e-04\nEpoch 6/100\n\u001b[1m103/103\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m96s\u001b[0m 928ms/step - accuracy: 0.5394 - loss: 10.9578 - val_accuracy: 0.6248 - val_loss: 10.3439 - learning_rate: 1.0000e-04\nEpoch 7/100\n\u001b[1m103/103\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m95s\u001b[0m 925ms/step - accuracy: 0.5444 - loss: 10.2660 - val_accuracy: 0.6377 - val_loss: 9.7046 - learning_rate: 1.0000e-04\nEpoch 8/100\n\u001b[1m 78/103\u001b[0m \u001b[32m━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━━━━━\u001b[0m \u001b[1m20s\u001b[0m 818ms/step - accuracy: 0.5629 - loss: 9.6834","output_type":"stream"}],"execution_count":null},{"cell_type":"code","source":"# 7. Evaluation\ny_pred = np.argmax(model.predict(X_test), axis=1)\n\nprint(\"\\nClassification Report:\")\nprint(classification_report(y_test, y_pred, target_names=CLASS_NAMES))\n\nplt.figure(figsize=(8,6))\nsns.heatmap(confusion_matrix(y_test, y_pred), \n            annot=True, fmt='d',\n            xticklabels=CLASS_NAMES,\n            yticklabels=CLASS_NAMES)\nplt.title('Confusion Matrix')\nplt.xlabel('Predicted')\nplt.ylabel('True')\nplt.show()\n","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# Save in H5 format\nmodel.save('skin_disease_classifier_augmented.h5')\nprint(\"Model saved successfully in H5 format!\")","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"\n# 9. Final Evaluation\nfrom tensorflow.keras.models import load_model\nimport numpy as np\nfrom sklearn.metrics import accuracy_score\n\n# Redefine custom preprocessing layer class (MUST match original definition)\nclass VGGPreprocess(tf.keras.layers.Layer):\n    def call(self, inputs):\n        return tf.keras.applications.vgg19.preprocess_input(inputs)\n\n# Load model with custom layer specification\nmodel = load_model(\n    'skin_disease_classifier_augmented.h5',\n    custom_objects={'VGGPreprocess': VGGPreprocess}\n)\n\n# Convert test data to proper format\nX_test_prepared = X_test.astype('float32')  # Ensure correct dtype\n\n# Evaluate using batched dataset (recommended for large datasets)\ntest_loss, test_accuracy = model.evaluate(\n    tf.data.Dataset.from_tensor_slices((X_test_prepared, y_test))\n                            .batch(BATCH_SIZE),\n    verbose=0\n)\n\nprint(f\"\\nFinal Test Accuracy: {test_accuracy * 100:.2f}%\")\nprint(f\"Final Test Loss: {test_loss:.4f}\")\n\n# Predict in batches to avoid memory issues\ny_pred_probs = model.predict(X_test_prepared, batch_size=BATCH_SIZE)\ny_pred_classes = np.argmax(y_pred_probs, axis=1)\n\n# Verify accuracy\nmanual_accuracy = accuracy_score(y_test, y_pred_classes)\nprint(f\"\\nManual Accuracy Verification: {manual_accuracy * 100:.2f}%\")\n\n# Allow small numerical differences due to floating point operations\nassert abs(test_accuracy - manual_accuracy) < 1e-4, \\\n    f\"Accuracy mismatch: {test_accuracy} vs {manual_accuracy}\"","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"","metadata":{"trusted":true},"outputs":[],"execution_count":null}]}